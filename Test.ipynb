{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лемматизация файла оценки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mystem import mystem as mstm\n",
    "import csv\n",
    "from collections import namedtuple\n",
    "\n",
    "def lemmatize(dataset, mode):\n",
    "\n",
    "    testFile = 'Result/' + dataset + '/' + mode + '.csv'\n",
    "    WordBag = namedtuple('WordBag', 'context_id word gold_sense_id positions context')\n",
    "    originList = list()\n",
    "\n",
    "    contextAll = ''\n",
    "\n",
    "    with open(testFile, 'r', encoding='utf-8', newline='') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        next(reader, None)  # skip the headers\n",
    "    \n",
    "        for row in reader:\n",
    "            word, context = row[1], remove_accents(row[5])\n",
    "\n",
    "            originList.append(\n",
    "                WordBag(context_id=int(row[0]),\n",
    "                        word=word,\n",
    "                        gold_sense_id=row[2],\n",
    "                        positions=row[4],\n",
    "                        context=context\n",
    "                        )\n",
    "            )\n",
    "    \n",
    "    contextDictClean = write_mystem_dict(originList)\n",
    "    \n",
    "    return contextDictClean, originList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_mystem_dict(originList):\n",
    "    \"\"\"Write dict [word, mystem_sentences_list]\"\"\"\n",
    "    contextDict = dict()\n",
    "    contextDictClean = dict()\n",
    "    contextDictCount = dict()\n",
    "\n",
    "    for row in originList:\n",
    "        if row.word not in contextDict:\n",
    "            contextDict[row.word] = str()\n",
    "            contextDictCount[row.word] = 0\n",
    "\n",
    "        contextDict[row.word] = contextDict[row.word] + row.context + ' \\n\\n '\n",
    "        contextDictCount[row.word] += 1\n",
    "\n",
    "    for word in contextDict:\n",
    "        contextList = mstm(contextDict[word])\n",
    "        contextDictClean[word] = lemmatized_text_list(contextList)\n",
    "        \n",
    "    for word in contextDict:\n",
    "        word_diff = contextDictCount[word] - len(contextDictClean[word])\n",
    "        if word_diff != 0:\n",
    "            print('ERROR in mystem:', word, word_diff)\n",
    "    \n",
    "    return contextDictClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    \"\"\"\n",
    "    Removes non-unicode symbols from string\n",
    "    \"\"\"\n",
    "    nfс_form = unicodedata.normalize('NFC', input_str)\n",
    "    nfс_form = re.sub(r'[^А-Яа-яЁё\\s\\-]', u'', nfс_form, flags=re.UNICODE)\n",
    "    return u\"\".join([c for c in nfс_form if not unicodedata.combining(c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделение лемм из выхлопа mystem\n",
    "def lemmatized_text_list(contextList):\n",
    "    \"\"\"\n",
    "    Gets lemmas from list of tuples (mystem func)\n",
    "    \"\"\"\n",
    "    line = ''\n",
    "    textList = list()\n",
    "    for sentence in contextList:\n",
    "        if len(sentence) == 0:\n",
    "            continue\n",
    "        for word in sentence:\n",
    "            if word[2] in ['UNKNOWN', 'CONJ', 'INTJ', 'PART', 'PR']:\n",
    "                continue\n",
    "            lemma = word[1]\n",
    "            if lemma != '.':\n",
    "                if len(lemma) > 1 and lemma[-1:] == '?':\n",
    "                    lemma = lemma[:-1]                     \n",
    "                line += lemma + ' '\n",
    "        textList.append(line)\n",
    "        line = ''\n",
    "    return textList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели и предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_results(trainwords, clf, printscreen=True, full=True):\n",
    "    \n",
    "    resultList = list()\n",
    "    clfDict = learn_clf(clf, printscreen, 'KNeighborsClassifier_MultinomialNB', full)\n",
    "    for word in trainwords:\n",
    "        if word in clfDict.keys():\n",
    "            context = contextDictClean[word]\n",
    "            clf = clfDict[word]\n",
    "            wordresult = clf.predict(context)\n",
    "            for result in wordresult:\n",
    "                resultList.append(result)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "def learn_clf(clf, printscreen, model_name, full=True):\n",
    "    if full == True:\n",
    "        path = 'Data/Expanded txt/{}({})/{}/'.format(dataset, mode, model_name)\n",
    "#         path = 'Input/full w2v txt/'\n",
    "    else:\n",
    "        path = 'Gold Standart/marked mystem/' + dataset + '(' + mode + ')/'\n",
    "    clfDict = dict()\n",
    "    for word in trainwords: \n",
    "        model = clone(clf)\n",
    "        clfDict[word] = learn_word(path, word, model)\n",
    "        \n",
    "        if printscreen == True:\n",
    "            print(word, clfDict[word].classes_)\n",
    "    return clfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def learn_word(path, word, model):\n",
    "    \"\"\"\n",
    "    Make word classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    clf = Pipeline([('tdidfvect', TfidfVectorizer(ngram_range=(1, 2))),\n",
    "                    ('model', model),\n",
    "                   ])\n",
    "    \n",
    "    contextTrain = list()\n",
    "    targetTrain = list() \n",
    "    \n",
    "    trainFile = path + word + '.csv'\n",
    "    with open(trainFile, 'r', encoding='utf-8', newline='') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for row in reader:\n",
    "            targetTrain.append(row[0])\n",
    "            contextTrain.append(row[1])\n",
    "    \n",
    "    clf.fit(contextTrain, targetTrain)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Запись результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_result_file(dataset, mode, learnModel, testModel, originList, resultList):\n",
    "\n",
    "    outputName = 'Result/' + dataset + '/' + mode + '.' + learnModel + '.' + testModel + '.csv'\n",
    "\n",
    "    with open(outputName, 'w', encoding='utf-8', newline='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
    "\n",
    "        wr.writerow(['context_id\\tword\\tgold_sense_id\\tpredict_sense_id\\tpositions\\tcontext'])\n",
    "        for index, row in enumerate(originList):\n",
    "            try:\n",
    "                line = '\\t'.join([\n",
    "                    str(row.context_id),\n",
    "                    row.word,\n",
    "                    str(row.gold_sense_id),\n",
    "                    str(resultList[index]),\n",
    "                    row.positions,\n",
    "                    row.context\n",
    "                ])\n",
    "            except:\n",
    "                continue\n",
    "            wr.writerow([line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вычисление результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluate2 as eval_score\n",
    "\n",
    "def result_score(semilearnModel, testModel, dataset, mode, printscreen=True):\n",
    "    file = 'Result/' + dataset + '/' + mode + '.' + semilearnModel + '.' + testModel + '.csv'\n",
    "    result = eval_score(file)\n",
    "    \n",
    "    if printscreen == True:\n",
    "        print(dataset, mode, semilearnModel, testModel)\n",
    "        print(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общая функция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(clf, trainwords, dataset, mode, learnModel, originList):\n",
    "    resultList = predict_results(trainwords, clf, printscreen=False)\n",
    "    testModel = str(clf.__class__.__name__)\n",
    "    \n",
    "    write_result_file(dataset, mode, learnModel, testModel, originList, resultList)\n",
    "    result_score(learnModel, testModel, dataset, mode, printscreen=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поиск оптимального алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предварительная обработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainwords = ['балка', 'вид', 'винт', 'горн', 'губа', 'жаба', 'клетка',\n",
    "              'крыло', 'купюра', 'курица', 'лавка', 'лайка', 'лев', 'лира',\n",
    "              'мина', 'мишень', 'обед', 'оклад', 'опушка', 'полис', 'пост', \n",
    "              'поток', 'проказа', 'пропасть', 'проспект', 'пытка', 'рысь',\n",
    "              'среда', 'хвост', 'штамп'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'bts-rnc'\n",
    "mode = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.4 s, sys: 340 ms, total: 1.74 s\n",
      "Wall time: 48.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Считывание и лемматизация файла проверки\n",
    "contextDictClean, originList = lemmatize(dataset, mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с моделями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "# from sklearn.naive_bayes import BernoulliNB\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На данных KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnModel = 'KNeighborsClassifier_MultinomialNB'\n",
    "mode = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train KNeighborsClassifier_MultinomialNB MultinomialNB\n",
      "0.308737\n",
      "CPU times: user 4.05 s, sys: 64.5 ms, total: 4.11 s\n",
      "Wall time: 4.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = MultinomialNB(alpha=0.01, fit_prior=False)\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод.** Частичное обучение на KNN + дальнейшее обучение на MNB дают результат 0.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Детальное изучение наилучшего алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\tari\tcount\n",
      "балка\t0.228342\t119\n",
      "вид\t0.246520\t77\n",
      "винт\t0.401767\t123\n",
      "горн\t0.127359\t51\n",
      "губа\t0.218559\t137\n",
      "жаба\t0.095849\t121\n",
      "клетка\t0.382173\t150\n",
      "крыло\t0.269338\t91\n",
      "купюра\t0.469913\t150\n",
      "курица\t0.067821\t93\n",
      "лавка\t0.084412\t149\n",
      "лайка\t0.607617\t99\n",
      "лев\t0.462464\t44\n",
      "лира\t-0.071571\t49\n",
      "мина\t0.234059\t65\n",
      "мишень\t0.107486\t121\n",
      "обед\t0.012241\t100\n",
      "оклад\t0.780554\t146\n",
      "опушка\t0.898969\t148\n",
      "полис\t0.471737\t142\n",
      "пост\t0.117120\t144\n",
      "поток\t-0.082392\t136\n",
      "проказа\t0.043743\t146\n",
      "пропасть\t0.356287\t127\n",
      "проспект\t0.534390\t139\n",
      "пытка\t0.199271\t143\n",
      "рысь\t0.593041\t120\n",
      "среда\t0.227137\t144\n",
      "хвост\t0.587657\t121\n",
      "штамп\t0.078159\t96\n",
      "\t0.308737\t3491\n"
     ]
    }
   ],
   "source": [
    "from evaluate import evaluate as eval_score_details\n",
    "eval_score_details('Result/bts-rnc/train.KNeighborsClassifier_MultinomialNB.MultinomialNB.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формирование тестового файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainwords = ['акция', 'баба', 'байка', 'бум', 'бычок', 'вал', 'газ', 'гвоздика',\n",
    "             'гипербола', 'град', 'гусеница', 'дождь', 'домино', 'забой', 'икра',\n",
    "             'кабачок', 'капот', 'карьер', 'кличка', 'ключ', 'кок', 'кольцо',\n",
    "             'концерт', 'котелок', 'крона', 'круп', 'кулак', 'лейка', 'лук',\n",
    "             'мандарин', 'ножка', 'опора', 'патрон', 'печать', 'пол', 'полоз',\n",
    "             'почерк', 'пробка', 'рак', 'рок', 'свет', 'секрет', 'скат', 'слог',\n",
    "             'стан', 'стопка', 'таз', 'такса', 'тюрьма', 'шах', 'шашка'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = 'bts-rnc'\n",
    "mode = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Считывание и лемматизация файла проверки\n",
    "contextDictClean, originList = lemmatize(dataset, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "learnModel = 'KNeighborsClassifier'\n",
    "mode = 'test'\n",
    "\n",
    "clf = MultinomialNB(alpha=0.01, fit_prior=False)\n",
    "testModel = str(clf.__class__.__name__)\n",
    "\n",
    "resultList = predict_results(trainwords, clf, printscreen=False)\n",
    "write_result_file(dataset, mode, learnModel, testModel, originList, resultList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Результат без частичного обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainwords = ['акция', 'баба', 'байка', 'бум', 'бычок', 'вал', 'газ', 'гвоздика',\n",
    "             'гипербола', 'град', 'гусеница', 'дождь', 'домино', 'забой', 'икра',\n",
    "             'кабачок', 'капот', 'карьер', 'кличка', 'ключ', 'кок', 'кольцо',\n",
    "             'концерт', 'котелок', 'крона', 'круп', 'кулак', 'лейка', 'лук',\n",
    "             'мандарин', 'ножка', 'опора', 'патрон', 'печать', 'пол', 'полоз',\n",
    "             'почерк', 'пробка', 'рак', 'рок', 'свет', 'секрет', 'скат', 'слог',\n",
    "             'стан', 'стопка', 'таз', 'такса', 'тюрьма', 'шах', 'шашка'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = 'bts-rnc'\n",
    "mode = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Считывание и лемматизация файла проверки\n",
    "contextDictClean, originList = lemmatize(dataset, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "акция\n",
      "баба\n",
      "байка\n",
      "бум\n",
      "бычок\n",
      "вал\n",
      "газ\n",
      "гвоздика\n",
      "гипербола\n",
      "град\n",
      "гусеница\n",
      "дождь\n",
      "домино\n",
      "забой\n",
      "икра\n",
      "кабачок\n",
      "капот\n",
      "карьер\n",
      "кличка\n",
      "ключ\n",
      "кок\n",
      "кольцо\n",
      "концерт\n",
      "котелок\n",
      "крона\n",
      "круп\n",
      "кулак\n",
      "лейка\n",
      "лук\n",
      "мандарин\n",
      "ножка\n",
      "опора\n",
      "патрон\n",
      "печать\n",
      "пол\n",
      "полоз\n",
      "почерк\n",
      "пробка\n",
      "рак\n",
      "рок\n",
      "свет\n",
      "секрет\n",
      "скат\n",
      "слог\n",
      "стан\n",
      "стопка\n",
      "таз\n",
      "такса\n",
      "тюрьма\n",
      "шах\n",
      "шашка\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "learnModel = 'original'\n",
    "mode = 'test'\n",
    "\n",
    "clf = MultinomialNB(alpha=0.01, fit_prior=False)\n",
    "testModel = str(clf.__class__.__name__)\n",
    "\n",
    "resultList = predict_results(trainwords, clf, printscreen=False, full=False)\n",
    "write_result_file(dataset, mode, learnModel, testModel, originList, resultList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Частичное обучение на тестовом файле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainwords = ['акция', 'баба', 'байка', 'бум', 'бычок', 'вал', 'газ', 'гвоздика',\n",
    "             'гипербола', 'град', 'гусеница', 'дождь', 'домино', 'забой', 'икра',\n",
    "             'кабачок', 'капот', 'карьер', 'кличка', 'ключ', 'кок', 'кольцо',\n",
    "             'концерт', 'котелок', 'крона', 'круп', 'кулак', 'лейка', 'лук',\n",
    "             'мандарин', 'ножка', 'опора', 'патрон', 'печать', 'пол', 'полоз',\n",
    "             'почерк', 'пробка', 'рак', 'рок', 'свет', 'секрет', 'скат', 'слог',\n",
    "             'стан', 'стопка', 'таз', 'такса', 'тюрьма', 'шах', 'шашка'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = 'bts-rnc'\n",
    "mode = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Считывание и лемматизация файла проверки\n",
    "contextDictClean, originList = lemmatize(dataset, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "learnModel = 'original'\n",
    "mode = 'test'\n",
    "\n",
    "clf = KNeighborsClassifier(weights='uniform')\n",
    "testModel = str(clf.__class__.__name__)\n",
    "\n",
    "# resultList = predict_results(trainwords, clf, printscreen=False, full=False)\n",
    "\n",
    "write_result_file(dataset, mode, learnModel, testModel, originList, resultList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_results(trainwords, clf, full=False):\n",
    "    \n",
    "    resultList = list()\n",
    "    clfDict = learn_clf(clf, printscreen, 'KNeighborsClassifier_MultinomialNB', full)\n",
    "    for word in trainwords:\n",
    "        if word in clfDict.keys():\n",
    "            context = contextDictClean[word]\n",
    "            \n",
    "            clf = clfDict[word]\n",
    "#             wordresult = clf.predict(context)\n",
    "            predicted = clf.predict_proba(context)\n",
    "            trainList, targetList, textList, count = semi_learn(clf, predicted, trainList, targetList, context)\n",
    "            \n",
    "            \n",
    "            for result in wordresult:\n",
    "                resultList.append(result)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def semi_learn(clf, predicted, trainList, targetList, textList):\n",
    "\n",
    "    startIndex = len(trainList)\n",
    "\n",
    "    for index, result in enumerate(predicted):\n",
    "        maximum = max(result)\n",
    "        if maximum >= 0.9:\n",
    "            label = np.argmax(result)\n",
    "            targetList.append(clf.classes_[label])\n",
    "            trainList.append(textList[index])\n",
    "\n",
    "    endIndex = len(trainList)\n",
    "\n",
    "    lenBefore = len(textList)\n",
    "\n",
    "    for index in range(startIndex, endIndex):\n",
    "        sentence = trainList[index]\n",
    "        if sentence in textList:\n",
    "            textList.remove(sentence)\n",
    "\n",
    "    lenAfter = len(textList)\n",
    "    \n",
    "    count = lenBefore - lenAfter\n",
    "    \n",
    "    return trainList, targetList, textList, count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
