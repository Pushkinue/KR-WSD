{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лемматизация файла оценки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mystem import mystem as mstm\n",
    "import csv\n",
    "from collections import namedtuple\n",
    "\n",
    "def lemmatize(dataset, mode):\n",
    "\n",
    "    testFile = 'Result/' + dataset + '/' + mode + '.csv'\n",
    "    WordBag = namedtuple('WordBag', 'context_id word gold_sense_id positions context')\n",
    "    originList = list()\n",
    "\n",
    "    contextAll = ''\n",
    "\n",
    "    with open(testFile, 'r', encoding='utf-8', newline='') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        next(reader, None)  # skip the headers\n",
    "    \n",
    "        for row in reader:\n",
    "            word, context = row[1], remove_accents(row[5])\n",
    "\n",
    "            originList.append(\n",
    "                WordBag(context_id=int(row[0]),\n",
    "                        word=word,\n",
    "                        gold_sense_id=row[2],\n",
    "                        positions=row[4],\n",
    "                        context=context\n",
    "                        )\n",
    "            )\n",
    "    \n",
    "    contextDictClean = write_mystem_dict(originList)\n",
    "    \n",
    "    return contextDictClean, originList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_mystem_dict(originList):\n",
    "    \"\"\"Write dict [word, mystem_sentences_list]\"\"\"\n",
    "    contextDict = dict()\n",
    "    contextDictClean = dict()\n",
    "    contextDictCount = dict()\n",
    "\n",
    "    for row in originList:\n",
    "        if row.word not in contextDict:\n",
    "            contextDict[row.word] = str()\n",
    "            contextDictCount[row.word] = 0\n",
    "\n",
    "        contextDict[row.word] = contextDict[row.word] + row.context + ' \\n\\n '\n",
    "        contextDictCount[row.word] += 1\n",
    "\n",
    "    for word in contextDict:\n",
    "        contextList = mstm(contextDict[word])\n",
    "        contextDictClean[word] = lemmatized_text_list(contextList)\n",
    "        \n",
    "    for word in contextDict:\n",
    "        word_diff = contextDictCount[word] - len(contextDictClean[word])\n",
    "        if word_diff != 0:\n",
    "            print('ERROR in mystem:', word, word_diff)\n",
    "    \n",
    "    return contextDictClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    \"\"\"\n",
    "    Removes non-unicode symbols from string\n",
    "    \"\"\"\n",
    "    nfс_form = unicodedata.normalize('NFC', input_str)\n",
    "    nfс_form = re.sub(r'[^А-Яа-яЁё\\s\\-]', u'', nfс_form, flags=re.UNICODE)\n",
    "    return u\"\".join([c for c in nfс_form if not unicodedata.combining(c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Выделение лемм из выхлопа mystem\n",
    "def lemmatized_text_list(contextList):\n",
    "    \"\"\"\n",
    "    Gets lemmas from list of tuples (mystem func)\n",
    "    \"\"\"\n",
    "    line = ''\n",
    "    textList = list()\n",
    "    for sentence in contextList:\n",
    "        if len(sentence) == 0:\n",
    "            continue\n",
    "        for word in sentence:\n",
    "            if word[2] in ['UNKNOWN', 'CONJ', 'INTJ', 'PART', 'PR']:\n",
    "                continue\n",
    "            lemma = word[1]\n",
    "            if lemma != '.':\n",
    "                if len(lemma) > 1 and lemma[-1:] == '?':\n",
    "                    lemma = lemma[:-1]                     \n",
    "                line += lemma + ' '\n",
    "        textList.append(line)\n",
    "        line = ''\n",
    "    return textList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели и предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_results(trainwords, clf, printscreen=True):\n",
    "    \n",
    "    resultList = list()\n",
    "    clfDict = learn_clf(clf, printscreen)\n",
    "    for word in trainwords:\n",
    "        if word in clfDict.keys():\n",
    "            context = contextDictClean[word]\n",
    "            clf = clfDict[word]\n",
    "            wordresult = clf.predict(context)\n",
    "            for result in wordresult:\n",
    "                resultList.append(result)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "def learn_clf(clf, printscreen):\n",
    "    path = 'Input/full txt/' + dataset + '(' + mode + ')/' + learnModel + '/'\n",
    "    clfDict = dict()\n",
    "    for word in trainwords: \n",
    "        model = clone(clf)\n",
    "        clfDict[word] = learn_word(path, word, model)\n",
    "        \n",
    "        if printscreen == True:\n",
    "            print(word, clfDict[word].classes_)\n",
    "    return clfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def learn_word(path, word, model):\n",
    "    \"\"\"\n",
    "    Make word classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    clf = Pipeline([('tdidfvect', TfidfVectorizer(ngram_range=(1, 2))),\n",
    "                    ('model', model),\n",
    "                   ])\n",
    "    \n",
    "    contextTrain = list()\n",
    "    targetTrain = list() \n",
    "    \n",
    "    trainFile = path + word + '.csv'\n",
    "    with open(trainFile, 'r', encoding='utf-8', newline='') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for row in reader:\n",
    "            targetTrain.append(row[0])\n",
    "            contextTrain.append(row[1])\n",
    "    \n",
    "    clf.fit(contextTrain, targetTrain)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Запись результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_result_file(dataset, mode, learnModel, testModel, originList, resultList):\n",
    "\n",
    "    outputName = 'Result/' + dataset + '/' + mode + '.' + learnModel + '.' + testModel + '.csv'\n",
    "\n",
    "    with open(outputName, 'w', encoding='utf-8', newline='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
    "\n",
    "        wr.writerow(['context_id\\tword\\tgold_sense_id\\tpredict_sense_id\\tpositions\\tcontext'])\n",
    "        for index, row in enumerate(originList):\n",
    "            try:\n",
    "                line = '\\t'.join([\n",
    "                    str(row.context_id),\n",
    "                    row.word,\n",
    "                    str(row.gold_sense_id),\n",
    "                    str(resultList[index]),\n",
    "                    row.positions,\n",
    "                    row.context\n",
    "                ])\n",
    "            except:\n",
    "                continue\n",
    "            wr.writerow([line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вычисление результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from evaluate import evaluate2 as eval_score\n",
    "\n",
    "def result_score(semilearnModel, testModel, dataset, mode, printscreen=True):\n",
    "    file = 'Result\\\\' + dataset + '\\\\' + mode + '.' + semilearnModel + '.' + testModel + '.csv'\n",
    "    result = eval_score(file)\n",
    "    \n",
    "    if printscreen == True:\n",
    "        print(dataset, mode, semilearnModel, testModel)\n",
    "        print(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общая функция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(clf, trainwords, dataset, mode, learnModel, originList):\n",
    "    resultList = predict_results(trainwords, clf, printscreen=False)\n",
    "    testModel = str(clf.__class__.__name__)\n",
    "    \n",
    "    write_result_file(dataset, mode, learnModel, testModel, originList, resultList)\n",
    "    result_score(learnModel, testModel, dataset, mode, printscreen=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поиск оптимального алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предварительная обработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainwords = ['балка',\n",
    "             'вид',\n",
    "             'винт',\n",
    "             'горн',\n",
    "             'губа',\n",
    "             'жаба',\n",
    "             'клетка',\n",
    "             'крыло',\n",
    "             'купюра', \n",
    "             'курица', \n",
    "             'лавка', \n",
    "             'лайка', \n",
    "             'лев', \n",
    "             'лира', \n",
    "             'мина', \n",
    "             'мишень', \n",
    "             'обед', \n",
    "             'оклад', \n",
    "             'опушка', \n",
    "             'полис', \n",
    "             'пост', \n",
    "             'поток', \n",
    "             'проказа', \n",
    "             'пропасть', \n",
    "             'проспект', \n",
    "             'пытка',\n",
    "             'рысь',\n",
    "             'среда',\n",
    "             'хвост',\n",
    "             'штамп',\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = 'bts-rnc'\n",
    "mode = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 55.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Считывание и лемматизация файла проверки\n",
    "contextDictClean, originList = lemmatize(dataset, mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с моделями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На данных KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learnModel = 'KNeighborsClassifier'\n",
    "mode = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train KNeighborsClassifier MultinomialNB\n",
      "0.302541\n",
      "Wall time: 4.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = MultinomialNB(alpha=0.01, fit_prior=False)\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train KNeighborsClassifier BernoulliNB\n",
      "0.236935\n",
      "Wall time: 3.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = BernoulliNB(alpha=0.01, fit_prior=False)\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train KNeighborsClassifier SGDClassifier\n",
      "0.248732\n",
      "Wall time: 5.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = SGDClassifier(max_iter=100, tol=0.01, loss='squared_hinge', penalty='elasticnet')\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train KNeighborsClassifier LinearSVC\n",
      "0.246685\n",
      "Wall time: 5.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, tol=0.0001, C=2.0)\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод.** Частичное обучение на KNN + дальнейшее обучение на MNB дают результат 0.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На данных MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learnModel = 'MultinomialNB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train MultinomialNB MultinomialNB\n",
      "0.178091\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = MultinomialNB(alpha=0.01, fit_prior=False)\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train MultinomialNB BernoulliNB\n",
      "0.235459\n",
      "Wall time: 726 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = BernoulliNB(alpha=0.01, fit_prior=False)\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train MultinomialNB SGDClassifier\n",
      "0.139709\n",
      "Wall time: 724 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = SGDClassifier(max_iter=100, tol=0.01, loss='squared_hinge', penalty='elasticnet')\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train MultinomialNB LinearSVC\n",
      "0.204999\n",
      "Wall time: 718 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, tol=0.0001, C=2.0)\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод.** MNB - не лучший способ частичного обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На данных Бернулли NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learnModel = 'BernoulliNB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train BernoulliNB MultinomialNB\n",
      "0.220795\n",
      "Wall time: 5.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = MultinomialNB(alpha=0.01, fit_prior=False)\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train BernoulliNB BernoulliNB\n",
      "0.139015\n",
      "Wall time: 5.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = BernoulliNB(alpha=0.01, fit_prior=False)\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train BernoulliNB SGDClassifier\n",
      "0.157445\n",
      "Wall time: 6.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = SGDClassifier(max_iter=100, tol=0.01, loss='squared_hinge', penalty='elasticnet')\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train BernoulliNB LinearSVC\n",
      "0.180712\n",
      "Wall time: 7.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, tol=0.0001, C=2.0)\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# На данных GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learnModel = 'GradientBoostingClassifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train GradientBoostingClassifier MultinomialNB\n",
      "0.228913\n",
      "Wall time: 4.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = MultinomialNB(alpha=0.01, fit_prior=False)\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train GradientBoostingClassifier BernoulliNB\n",
      "0.164874\n",
      "Wall time: 4.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = BernoulliNB(alpha=0.01, fit_prior=False)\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train GradientBoostingClassifier SGDClassifier\n",
      "0.183972\n",
      "Wall time: 4.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = SGDClassifier(max_iter=100, tol=0.01, loss='squared_hinge', penalty='elasticnet')\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train GradientBoostingClassifier LinearSVC\n",
      "0.203076\n",
      "Wall time: 5.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, tol=0.0001, C=2.0)\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же были использованы, но по разным причинам не включены в ноутбук: GradientBoostingClassifier (время), AdaBoostClassifier (точность), KNeighborsClassifier (точность), RandomForestClassifier (точность)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Детальное изучение наилучшего алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\tari\tcount\n",
      "балка\t0.228342\t119\n",
      "вид\t0.246520\t77\n",
      "винт\t0.401767\t123\n",
      "горн\t0.127359\t51\n",
      "губа\t0.218559\t137\n",
      "жаба\t0.103429\t121\n",
      "клетка\t0.382173\t150\n",
      "крыло\t0.269338\t91\n",
      "купюра\t0.469913\t150\n",
      "курица\t0.067821\t93\n",
      "лавка\t0.084412\t149\n",
      "лайка\t0.607617\t99\n",
      "лев\t0.462464\t44\n",
      "лира\t-0.037962\t49\n",
      "мина\t0.234059\t65\n",
      "мишень\t0.107486\t121\n",
      "обед\t0.012241\t100\n",
      "оклад\t0.780554\t146\n",
      "опушка\t0.898969\t148\n",
      "полис\t0.471737\t142\n",
      "пост\t0.131352\t144\n",
      "поток\t-0.082392\t136\n",
      "проказа\t0.043743\t146\n",
      "пропасть\t0.155421\t127\n",
      "проспект\t0.534390\t139\n",
      "пытка\t0.199271\t143\n",
      "рысь\t0.593041\t120\n",
      "среда\t0.222034\t144\n",
      "хвост\t0.587657\t121\n",
      "штамп\t0.078159\t96\n",
      "\t0.302541\t3491\n"
     ]
    }
   ],
   "source": [
    "from evaluate import evaluate as eval_score_details\n",
    "eval_score_details('Result\\\\bts-rnc\\\\train.KNeighborsClassifier.MultinomialNB.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формирование тестового файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainwords = [\n",
    "    'акция',\n",
    "    'баба',\n",
    "    'байка',\n",
    "    'бум',\n",
    "    'бычок',\n",
    "    'вал',\n",
    "    'газ',\n",
    "    'гвоздика',\n",
    "    'гипербола', \n",
    "    'град',\n",
    "    'гусеница', \n",
    "    'дождь', \n",
    "    'домино', \n",
    "    'забой', \n",
    "    'икра', \n",
    "    'кабачок',\n",
    "    'капот', \n",
    "    'карьер', \n",
    "    'кличка', \n",
    "    'ключ', \n",
    "    'кок', \n",
    "    'кольцо', \n",
    "    'концерт', \n",
    "    'котелок', \n",
    "    'крона', \n",
    "    'круп',\n",
    "    'кулак',\n",
    "    'лейка',\n",
    "    'лук',\n",
    "    'мандарин',\n",
    "    'ножка', \n",
    "    'опора', \n",
    "    'патрон', \n",
    "    'печать', \n",
    "    'пол',\n",
    "    'полоз', \n",
    "    'почерк', \n",
    "    'пробка', \n",
    "    'рак', \n",
    "    'рок', \n",
    "    'свет', \n",
    "    'секрет', \n",
    "    'скат', \n",
    "    'слог', \n",
    "    'стан',\n",
    "    'стопка',\n",
    "    'таз',\n",
    "    'такса',\n",
    "    'тюрьма',\n",
    "    'шах',\n",
    "    'шашка'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = 'bts-rnc'\n",
    "mode = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Считывание и лемматизация файла проверки\n",
    "contextDictClean, originList = lemmatize(dataset, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Input/full txt/bts-rnc(test)/KNeighborsClassifier/балка.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-14bf52603c2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtestModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mresultList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprintscreen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mwrite_result_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearnModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresultList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-feb64ae256da>\u001b[0m in \u001b[0;36mpredict_results\u001b[1;34m(trainwords, clf, printscreen)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mresultList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mclfDict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearn_clf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprintscreen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclfDict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-97e077b70f53>\u001b[0m in \u001b[0;36mlearn_clf\u001b[1;34m(clf, printscreen)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mclfDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearn_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprintscreen\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-ccb914cc6739>\u001b[0m in \u001b[0;36mlearn_word\u001b[1;34m(path, word, model)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mtrainFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mword\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQUOTE_NONE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Input/full txt/bts-rnc(test)/KNeighborsClassifier/балка.csv'"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "learnModel = 'KNeighborsClassifier'\n",
    "mode = 'test'\n",
    "\n",
    "clf = MultinomialNB(alpha=0.01, fit_prior=False)\n",
    "testModel = str(clf.__class__.__name__)\n",
    "\n",
    "resultList = predict_results(trainwords, clf, printscreen=False)\n",
    "write_result_file(dataset, mode, learnModel, testModel, originList, resultList)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
