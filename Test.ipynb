{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лемматизация файла оценки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mystem import mystem as mstm\n",
    "import csv\n",
    "from collections import namedtuple\n",
    "\n",
    "def lemmatize(dataset, mode):\n",
    "\n",
    "    testFile = 'Result/' + dataset + '/' + mode + '.csv'\n",
    "    WordBag = namedtuple('WordBag', 'context_id word gold_sense_id positions context')\n",
    "    originList = list()\n",
    "\n",
    "    contextAll = ''\n",
    "\n",
    "    with open(testFile, 'r', encoding='utf-8', newline='') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        next(reader, None)  # skip the headers\n",
    "    \n",
    "        for row in reader:\n",
    "            word, context = row[1], remove_accents(row[5])\n",
    "\n",
    "            originList.append(\n",
    "                WordBag(context_id=int(row[0]),\n",
    "                        word=word,\n",
    "                        gold_sense_id=row[2],\n",
    "                        positions=row[4],\n",
    "                        context=context\n",
    "                        )\n",
    "            )\n",
    "    \n",
    "    contextDictClean = write_mystem_dict(originList)\n",
    "    \n",
    "    return contextDictClean, originList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_mystem_dict(originList):\n",
    "    \"\"\"Write dict [word, mystem_sentences_list]\"\"\"\n",
    "    contextDict = dict()\n",
    "    contextDictClean = dict()\n",
    "    contextDictCount = dict()\n",
    "\n",
    "    for row in originList:\n",
    "        if row.word not in contextDict:\n",
    "            contextDict[row.word] = str()\n",
    "            contextDictCount[row.word] = 0\n",
    "\n",
    "        contextDict[row.word] = contextDict[row.word] + row.context + ' \\n\\n '\n",
    "        contextDictCount[row.word] += 1\n",
    "\n",
    "    for word in contextDict:\n",
    "        contextList = mstm(contextDict[word])\n",
    "        contextDictClean[word] = lemmatized_text_list(contextList)\n",
    "        \n",
    "    for word in contextDict:\n",
    "        word_diff = contextDictCount[word] - len(contextDictClean[word])\n",
    "        if word_diff != 0:\n",
    "            print('ERROR in mystem:', word, word_diff)\n",
    "    \n",
    "    return contextDictClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    \"\"\"\n",
    "    Removes non-unicode symbols from string\n",
    "    \"\"\"\n",
    "    nfс_form = unicodedata.normalize('NFC', input_str)\n",
    "    nfс_form = re.sub(r'[^А-Яа-яЁё\\s\\-]', u'', nfс_form, flags=re.UNICODE)\n",
    "    return u\"\".join([c for c in nfс_form if not unicodedata.combining(c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Выделение лемм из выхлопа mystem\n",
    "def lemmatized_text_list(contextList):\n",
    "    \"\"\"\n",
    "    Gets lemmas from list of tuples (mystem func)\n",
    "    \"\"\"\n",
    "    line = ''\n",
    "    textList = list()\n",
    "    for sentence in contextList:\n",
    "        if len(sentence) == 0:\n",
    "            continue\n",
    "        for word in sentence:\n",
    "            if word[2] in ['UNKNOWN', 'CONJ', 'INTJ', 'PART', 'PR']:\n",
    "                continue\n",
    "            lemma = word[1]\n",
    "            if lemma != '.':\n",
    "                if len(lemma) > 1 and lemma[-1:] == '?':\n",
    "                    lemma = lemma[:-1]                     \n",
    "                line += lemma + ' '\n",
    "        textList.append(line)\n",
    "        line = ''\n",
    "    return textList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели и предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_results(trainwords, clf, printscreen=True):\n",
    "    \n",
    "    resultList = list()\n",
    "    clfDict = learn_clf(clf, printscreen)\n",
    "    for word in trainwords:\n",
    "        if word in clfDict.keys():\n",
    "            context = contextDictClean[word]\n",
    "            clf = clfDict[word]\n",
    "            wordresult = clf.predict(context)\n",
    "            for result in wordresult:\n",
    "                resultList.append(result)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return resultList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "def learn_clf(clf, printscreen):\n",
    "    path = 'Data/НКРЯ/labeled txt/' + dataset + '(' + mode + ')/' + learnModel + '/'\n",
    "    clfDict = dict()\n",
    "    for word in trainwords: \n",
    "        model = clone(clf)\n",
    "        clfDict[word] = learn_word(path, word, model)\n",
    "        \n",
    "        if printscreen == True:\n",
    "            print(word, clfDict[word].classes_)\n",
    "    return clfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def learn_word(path, word, model):\n",
    "    \"\"\"\n",
    "    Make word classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    clf = Pipeline([('tdidfvect', TfidfVectorizer(ngram_range=(1, 2))),\n",
    "                    ('model', model),\n",
    "                   ])\n",
    "    \n",
    "    contextTrain = list()\n",
    "    targetTrain = list() \n",
    "    \n",
    "    trainFile = path + word + '.csv'\n",
    "    with open(trainFile, 'r', encoding='utf-8', newline='') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for row in reader:\n",
    "            targetTrain.append(row[0])\n",
    "            contextTrain.append(row[1])\n",
    "    \n",
    "    clf.fit(contextTrain, targetTrain)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Запись результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_result_file(dataset, mode, learnModel, testModel, originList, resultList):\n",
    "\n",
    "    outputName = 'Result/' + dataset + '/' + mode + '.' + learnModel + '.' + testModel + '.csv'\n",
    "\n",
    "    with open(outputName, 'w', encoding='utf-8', newline='') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
    "\n",
    "        wr.writerow(['context_id\\tword\\tgold_sense_id\\tpredict_sense_id\\tpositions\\tcontext'])\n",
    "        for index, row in enumerate(originList):\n",
    "            try:\n",
    "                line = '\\t'.join([\n",
    "                    str(row.context_id),\n",
    "                    row.word,\n",
    "                    str(row.gold_sense_id),\n",
    "                    str(resultList[index]),\n",
    "                    row.positions,\n",
    "                    row.context\n",
    "                ])\n",
    "            except:\n",
    "                continue\n",
    "            wr.writerow([line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вычисление результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from evaluate import evaluate2 as eval_score\n",
    "\n",
    "def result_score(semilearnModel, testModel, dataset, mode, printscreen=True):\n",
    "    file = 'Result\\\\' + dataset + '\\\\' + mode + '.' + semilearnModel + '.' + testModel + '.csv'\n",
    "    result = eval_score(file)\n",
    "    \n",
    "    if printscreen == True:\n",
    "        print(dataset, mode, semilearnModel, testModel)\n",
    "        print(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn import model_selection\n",
    "\n",
    "# def find_best_params(model, parameters_grid, df):\n",
    "    \n",
    "#     classifier = Pipeline(\n",
    "#         [('tfidf', TfidfVectorizer()),\n",
    "#          ('clf', model),\n",
    "#         ])   \n",
    "    \n",
    "#     # Деление данных на обучающую и тестовую выборки\n",
    "#     train_data, test_data, train_labels, test_labels = model_selection.train_test_split(df.name, df.tare, \n",
    "#                                                                                      test_size = 0.3,random_state = 0)\n",
    "         \n",
    "#     # Обучаем grid search\n",
    "#     grid_cv = model_selection.GridSearchCV(classifier, parameters_grid, scoring = 'accuracy')\n",
    "#     grid_cv.fit(train_data, train_labels)\n",
    "    \n",
    "#     print('best score:', grid_cv.best_score_, '\\n')\n",
    "#     print('best parametes:', grid_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общая функция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(clf, trainwords, dataset, mode, learnModel, originList):\n",
    "    resultList = predict_results(trainwords, clf, printscreen=False)\n",
    "    testModel = str(clf.__class__.__name__)\n",
    "    \n",
    "    write_result_file(dataset, mode, learnModel, testModel, originList, resultList)\n",
    "    result_score(learnModel, testModel, dataset, mode, printscreen=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Начало работы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предварительная обработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trainwords = ['балка',\n",
    "#              'вид',\n",
    "#              'винт',\n",
    "#              'горн',\n",
    "#              'губа',\n",
    "#              'жаба',\n",
    "#              'клетка',\n",
    "#              'крыло',\n",
    "#              'купюра', \n",
    "#              'курица', \n",
    "#              'лавка', \n",
    "#              'лайка', \n",
    "#              'лев', \n",
    "#              'лира', \n",
    "#              'мина', \n",
    "#              'мишень', \n",
    "#              'обед', \n",
    "#              'оклад', \n",
    "#              'опушка', \n",
    "#              'полис', \n",
    "#              'пост', \n",
    "#              'поток', \n",
    "#              'проказа', \n",
    "#              'пропасть', \n",
    "#              'проспект', \n",
    "#              'пытка',\n",
    "#              'рысь',\n",
    "#              'среда',\n",
    "#              'хвост',\n",
    "#              'штамп',\n",
    "#             ]\n",
    "\n",
    "trainwords = [\n",
    "    'акция',\n",
    "    'баба',\n",
    "    'байка',\n",
    "    'бум',\n",
    "    'бычок',\n",
    "    'вал',\n",
    "    'газ',\n",
    "    'гвоздика',\n",
    "    'гипербола', \n",
    "    'град',\n",
    "    'гусеница', \n",
    "    'дождь', \n",
    "    'домино', \n",
    "    'забой', \n",
    "    'икра', \n",
    "    'кабачок',\n",
    "    'капот', \n",
    "    'карьер', \n",
    "    'кличка', \n",
    "    'ключ', \n",
    "    'кок', \n",
    "    'кольцо', \n",
    "    'концерт', \n",
    "    'котелок', \n",
    "    'крона', \n",
    "    'круп',\n",
    "    'кулак',\n",
    "    'лейка',\n",
    "    'лук',\n",
    "    'мандарин',\n",
    "    'ножка', \n",
    "    'опора', \n",
    "    'патрон', \n",
    "    'печать', \n",
    "    'пол',\n",
    "    'полоз', \n",
    "    'почерк', \n",
    "    'пробка', \n",
    "    'рак', \n",
    "    'рок', \n",
    "    'свет', \n",
    "    'секрет', \n",
    "    'скат', \n",
    "    'слог', \n",
    "    'стан',\n",
    "    'стопка',\n",
    "    'таз',\n",
    "    'такса',\n",
    "    'тюрьма',\n",
    "    'шах',\n",
    "    'шашка'\n",
    "] \n",
    "\n",
    "dataset = 'bts-rnc'\n",
    "mode = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Считывание и лемматизация файла проверки\n",
    "contextDictClean, originList = lemmatize(dataset, mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестовый файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "learnModel = 'KNeighborsClassifier'\n",
    "\n",
    "clf = MultinomialNB(alpha=0.01, fit_prior=False)\n",
    "testModel = str(clf.__class__.__name__)\n",
    "\n",
    "resultList = predict_results(trainwords, clf, printscreen=False)\n",
    "write_result_file(dataset, mode, learnModel, testModel, originList, resultList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с моделями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На данных KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learnModel = 'KNeighborsClassifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train KNeighborsClassifier MultinomialNB\n",
      "0.302541\n",
      "Wall time: 3.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = MultinomialNB(alpha=0.01, fit_prior=False)\n",
    "#alpha=0.01, fit_prior=False - 0.301357\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train KNeighborsClassifier BernoulliNB\n",
      "0.236935\n",
      "Wall time: 3.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = BernoulliNB(alpha=0.01, fit_prior=False)\n",
    "#alpha=0.01, fit_prior=False - 0.236203\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train KNeighborsClassifier SGDClassifier\n",
      "0.231198\n",
      "Wall time: 4.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = SGDClassifier(max_iter=100, tol=0.01, loss='squared_hinge', penalty='elasticnet')\n",
    "#max_iter=100, tol=0.01, loss='squared_hinge', penalty='elasticnet' - 0.250178\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train KNeighborsClassifier LinearSVC\n",
      "0.247301\n",
      "Wall time: 4.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, tol=0.0001, C=2.0)\n",
    "#penalty='l1', loss='squared_hinge', dual=False, C=2.0 - 0.24\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод.** Частичное обучение на KNN + дальнейшее обучение на MNB дают результат 0.3.\n",
    "\n",
    "Так же были использованы, но по разным причинам не включены в ноутбук: GradientBoostingClassifier (время), AdaBoostClassifier (точность), KNeighborsClassifier (точность), RandomForestClassifier (точность)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На данных MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learnModel = 'MultinomialNB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train MultinomialNB MultinomialNB\n",
      "0.178091\n",
      "Wall time: 650 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = MultinomialNB(alpha=0.01, fit_prior=False)\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train MultinomialNB BernoulliNB\n",
      "0.235459\n",
      "Wall time: 642 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = BernoulliNB(alpha=0.01, fit_prior=False)\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train MultinomialNB SGDClassifier\n",
      "0.160804\n",
      "Wall time: 671 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = SGDClassifier(max_iter=100, tol=0.01, loss='squared_hinge', penalty='elasticnet')\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train MultinomialNB LinearSVC\n",
      "0.206220\n",
      "Wall time: 758 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, tol=0.0001, C=2.0)\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод.** MNB - не лучший способ частичного обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На данных Бернулли NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learnModel = 'BernoulliNB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train BernoulliNB MultinomialNB\n",
      "0.220795\n",
      "Wall time: 4.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = MultinomialNB(alpha=0.01, fit_prior=False)\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train BernoulliNB BernoulliNB\n",
      "0.139015\n",
      "Wall time: 4.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = BernoulliNB(alpha=0.01, fit_prior=False)\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train BernoulliNB SGDClassifier\n",
      "0.147161\n",
      "Wall time: 6.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = SGDClassifier(max_iter=100, tol=0.01, loss='squared_hinge', penalty='elasticnet')\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train BernoulliNB LinearSVC\n",
      "0.180712\n",
      "Wall time: 6.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, tol=0.0001, C=2.0)\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# На данных GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learnModel = 'GradientBoostingClassifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train GradientBoostingClassifier MultinomialNB\n",
      "0.228913\n",
      "Wall time: 4.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = MultinomialNB(alpha=0.01, fit_prior=False)\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train GradientBoostingClassifier BernoulliNB\n",
      "0.164874\n",
      "Wall time: 4.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = BernoulliNB(alpha=0.01, fit_prior=False)\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train GradientBoostingClassifier SGDClassifier\n",
      "0.184168\n",
      "Wall time: 5.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = SGDClassifier(max_iter=100, tol=0.01, loss='squared_hinge', penalty='elasticnet')\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bts-rnc train GradientBoostingClassifier LinearSVC\n",
      "0.202593\n",
      "Wall time: 5.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LinearSVC(penalty='l1', loss='squared_hinge', dual=False, tol=0.0001, C=2.0)\n",
    "test_model(clf, trainwords, dataset, mode, learnModel, originList)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
