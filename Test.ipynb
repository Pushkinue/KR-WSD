{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mystem import mystem as mstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    \"\"\"\n",
    "    Removes non-unicode symbols from string\n",
    "    \"\"\"\n",
    "    nfс_form = unicodedata.normalize('NFC', input_str)\n",
    "    nfс_form = re.sub(r'[^А-Яа-яЁё\\s\\-]', u'', nfс_form, flags=re.UNICODE)\n",
    "    return u\"\".join([c for c in nfс_form if not unicodedata.combining(c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Выделение лемм из выхлопа mystem\n",
    "def lemmatized_text_list(contextList):\n",
    "    \"\"\"\n",
    "    Gets lemmas from list of tuples (mystem func)\n",
    "    \"\"\"\n",
    "    line = ''\n",
    "    textList = list()\n",
    "    for sentence in contextList:\n",
    "        if len(sentence) == 0:\n",
    "            continue\n",
    "        for word in sentence:\n",
    "            if word[2] in ['UNKNOWN', 'CONJ', 'INTJ', 'PART', 'PR']:\n",
    "                continue\n",
    "            lemma = word[1]\n",
    "            if lemma != '.':\n",
    "                if len(lemma) > 1 and lemma[-1:] == '?':\n",
    "                    lemma = lemma[:-1]                     \n",
    "                line += lemma + ' '\n",
    "        textList.append(line)\n",
    "        line = ''\n",
    "    return textList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printresult(word, context, clf):\n",
    "    predicted = clf.predict(context)\n",
    "    print(word, len(predicted))\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def learn_word(path, word, clf):\n",
    "    \"\"\"\n",
    "    Make word classifier\n",
    "    \"\"\"\n",
    "    contextTrain = list()\n",
    "    targetTrain = list() \n",
    "    \n",
    "    trainFile = path + word + '.csv'\n",
    "    with open(trainFile, 'r', encoding='utf-8', newline='') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for row in reader:\n",
    "            targetTrain.append(row[0])\n",
    "            contextTrain.append(row[1])\n",
    "    \n",
    "    clf.fit(contextTrain, targetTrain)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.naive_bayes import BernoulliNB\n",
    "# from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.linear_model import Perceptron\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(solver='lbfgs', alpha=0.001, max_iter=1000, tol=0.001)\n",
    "\n",
    "# from sklearn.svm import LinearSVC\n",
    "# model = LinearSVC() \n",
    "\n",
    "clf = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('model', model),\n",
    "               ])\n",
    "\n",
    "\n",
    "# model = Perceptron(penalty=None,\n",
    "#                  alpha=0.0001, \n",
    "#                  fit_intercept=True, \n",
    "#                  max_iter=30000, \n",
    "#                  tol=0.1, \n",
    "#                  shuffle=True,   \n",
    "#                 ).fit(X_train_tfidf, targetList) # 0.459148    \n",
    "          \n",
    "     \n",
    "# model = SGDClassifier(loss=\"hinge\",\n",
    "#                     penalty=\"l2\",\n",
    "#                     max_iter=10000,\n",
    "#                     alpha=0.00001,\n",
    "#                    ).fit(X_train_tfidf, targetList) # 0.374446\n",
    "\n",
    "# model = NearestCentroid().fit(X_train_tfidf, targetList) # 0.377511\n",
    "# model = MultinomialNB(alpha=0.1, fit_prior=False).fit(X_train_tfidf, targetList) # 0.434186\n",
    "# model = BernoulliNB(alpha=0.1, fit_prior=False).fit(X_train_tfidf, targetList) # 0.383812\n",
    "# model = MLPClassifier(solver='lbfgs', alpha=0.001, max_iter=1000, tol=0.001).fit(X_train_tfidf, targetList) # 0.450178\n",
    "# model = GradientBoostingClassifier(n_estimators=5000).fit(X_train_tfidf, targetList) # 0.384457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trainwords = ['замок', 'лук', 'суда', 'бор']\n",
    "\n",
    "trainwords = ['балка',\n",
    "             'вид',\n",
    "             'винт',\n",
    "             'горн',\n",
    "             'губа',\n",
    "             'жаба',\n",
    "             'клетка',\n",
    "             'крыло',\n",
    "             'купюра', \n",
    "             'курица', \n",
    "             'лавка', \n",
    "             'лайка', \n",
    "             'лев', \n",
    "             'лира', \n",
    "             'мина', \n",
    "             'мишень', \n",
    "             'обед', \n",
    "             'оклад', \n",
    "             'опушка', \n",
    "             'полис', \n",
    "             'пост', \n",
    "             'поток', \n",
    "             'проказа', \n",
    "             'пропасть', \n",
    "             'проспект', \n",
    "             'пытка',\n",
    "             'рысь',\n",
    "             'среда',\n",
    "             'хвост',\n",
    "             'штамп',\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "балка ['34297' '39329']\n",
      "вид ['16525' '18983' '38473' '46934']\n",
      "винт ['16398' '17916' '32507' '39939']\n",
      "горн ['0' '1' '30349' '32374']\n",
      "губа ['32217' '36563']\n",
      "жаба [' 38123' '0' '1' '22390' '38123']\n",
      "клетка ['15531' '15764' '16575' '19864']\n",
      "крыло ['0' '19801' '20531' '29548' '31524' '35257' '36977']\n",
      "купюра [' 12767' '12767' '25844']\n",
      "курица ['33131' '38375']\n",
      "лавка ['38881']\n",
      "лайка ['21481' '30243']\n",
      "лев ['0' '1' '28022' '39252']\n",
      "лира ['0' '34938']\n",
      "мина ['0' '14200' '23313']\n",
      "мишень ['28103' '35543']\n",
      "обед [' 42118' '20613' '27747' '32048' '42118']\n",
      "оклад ['20522' '25098' '29484']\n",
      "опушка ['16750' '40487']\n",
      "полис ['0' '20510']\n",
      "пост ['19615' '21584' '22320' '36343']\n",
      "поток ['41114']\n",
      "проказа ['24456' '37005']\n",
      "пропасть ['0' '1' '38487' '39536']\n",
      "проспект ['0' '1' '13284' '28382']\n",
      "пытка ['24806' '28262']\n",
      "рысь ['20083' '36983']\n",
      "среда ['14872' '27906' '37772' '41363']\n",
      "хвост [' 18002' '0' '12982' '14619' '18002' '40730']\n",
      "штамп [' 30859' '30859' '34398' '35716' '39926']\n",
      "['34297' '39329']\n"
     ]
    }
   ],
   "source": [
    "path = 'Data/НКРЯ/labeled txt/'\n",
    "clfDict = dict()\n",
    "for word in trainwords:\n",
    "    \n",
    "    model = MLPClassifier(solver='lbfgs', alpha=0.001, max_iter=1000, tol=0.001)\n",
    "    \n",
    "    clf = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('model', model),\n",
    "                   ])\n",
    "    \n",
    "    clfDict[word] = learn_word(path, word, clf)\n",
    "    print(word, clfDict[word].classes_)\n",
    "    \n",
    "print(clfDict['балка'].classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import namedtuple\n",
    "\n",
    "testFile = 'Result//bts-rnc//train.csv'\n",
    "\n",
    "WordBag = namedtuple('WordBag', 'context_id word gold_sense_id positions context')\n",
    "originList = list()\n",
    "\n",
    "contextAll = ''\n",
    "lastWord = trainwords[0]\n",
    "\n",
    "with open(testFile, 'r', encoding='utf-8', newline='') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    next(reader, None)  # skip the headers\n",
    "    \n",
    "    for row in reader:\n",
    "        word, context = row[1], remove_accents(row[5])\n",
    "        \n",
    "        originList.append(\n",
    "            WordBag(context_id=int(row[0]),\n",
    "                    word=word,\n",
    "                    gold_sense_id=row[2],\n",
    "                    positions=row[4],\n",
    "                    context=context\n",
    "                    )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "contextDict = dict()\n",
    "contextDictClean = dict()\n",
    "contextDictCount = dict()\n",
    "\n",
    "for row in originList:\n",
    "    \n",
    "    if row.word not in contextDict:\n",
    "        contextDict[row.word] = str()\n",
    "        contextDictCount[row.word] = 0\n",
    "        \n",
    "    contextDict[row.word] = contextDict[row.word] + row.context + ' \\n\\n '\n",
    "    contextDictCount[row.word] += 1\n",
    "    \n",
    "for word in contextDict:\n",
    "    contextList = mstm(contextDict[word])\n",
    "    contextDictClean[word] = lemmatized_text_list(contextList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "балка 119 119\n",
      "вид 77 77\n",
      "винт 123 123\n",
      "горн 51 51\n",
      "губа 137 137\n",
      "жаба 121 121\n",
      "клетка 150 150\n",
      "крыло 91 91\n",
      "купюра 150 150\n",
      "курица 93 93\n",
      "лавка 149 149\n",
      "лайка 99 99\n",
      "лев 44 44\n",
      "лира 49 49\n",
      "мина 65 65\n",
      "мишень 121 121\n",
      "обед 100 100\n",
      "оклад 146 146\n",
      "опушка 148 148\n",
      "полис 142 142\n",
      "пост 144 144\n",
      "поток 136 136\n",
      "проказа 146 146\n",
      "пропасть 127 127\n",
      "проспект 139 139\n",
      "пытка 143 143\n",
      "рысь 120 120\n",
      "среда 144 144\n",
      "хвост 121 121\n",
      "штамп 96 96\n"
     ]
    }
   ],
   "source": [
    "for word in contextDict:\n",
    "    print(word, contextDictCount[word], len(contextDictClean[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "балка 119\n",
      "вид 77\n",
      "винт 123\n",
      "горн 51\n",
      "губа 137\n",
      "жаба 121\n",
      "клетка 150\n",
      "крыло 91\n",
      "купюра 150\n",
      "курица 93\n",
      "лавка 149\n",
      "лайка 99\n",
      "лев 44\n",
      "лира 49\n",
      "мина 65\n",
      "мишень 121\n",
      "обед 100\n",
      "оклад 146\n",
      "опушка 148\n",
      "полис 142\n",
      "пост 144\n",
      "поток 136\n",
      "проказа 146\n",
      "пропасть 127\n",
      "проспект 139\n",
      "пытка 143\n",
      "рысь 120\n",
      "среда 144\n",
      "хвост 121\n",
      "штамп 96\n",
      "3491\n"
     ]
    }
   ],
   "source": [
    "resultList = list()\n",
    "\n",
    "for word in trainwords:\n",
    "    if word in clfDict.keys():\n",
    "        context = contextDictClean[word]\n",
    "        clf = clfDict[word]\n",
    "        wordresult = printresult(word, context, clf)\n",
    "        for result in wordresult:\n",
    "            resultList.append(result)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "print(len(resultList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do:\n",
    "# При записи приходится использовать escapechar для кавычек, попробовать исправить\n",
    "outputName = 'Result//bts-rnc//train.KRWSD.csv'\n",
    "\n",
    "with open(outputName, 'w', encoding='utf-8', newline='') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
    "\n",
    "    wr.writerow(['context_id\\tword\\tgold_sense_id\\tpredict_sense_id\\tpositions\\tcontext'])\n",
    "    for index, row in enumerate(originList):\n",
    "        try:\n",
    "            line = '\\t'.join([\n",
    "                str(row.context_id),\n",
    "                row.word,\n",
    "                str(row.gold_sense_id),\n",
    "#                 str(resultList[row.context_id - 1]),\n",
    "                str(resultList[index]),\n",
    "                row.positions,\n",
    "                row.context\n",
    "            ])\n",
    "        except:\n",
    "            continue\n",
    "        wr.writerow([line])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
