{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'bts-rnc'\n",
    "mode = 'train'\n",
    "\n",
    "wordList = ['балка', 'вид', 'винт', 'горн', 'губа', 'жаба', 'клетка',\n",
    "            'крыло', 'купюра', 'курица', 'лавка', 'лайка', 'лев', 'лира',\n",
    "            'мина', 'мишень', 'обед', 'оклад', 'опушка', 'полис', 'пост', \n",
    "            'поток', 'проказа', 'пропасть', 'проспект', 'пытка', 'рысь',\n",
    "#             'поток', 'проказа', 'проспект', 'пытка', 'рысь',\n",
    "            'среда', 'хвост', 'штамп'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'Data\\\\Original\\\\Lemmatized\\\\' + dataset + '(' + mode + ')\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vectorize(sentence):\n",
    "    \"\"\"Calculate vector for a sentence.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    sentence - (list) list of tokens.\n",
    "    \"\"\"\n",
    "    sentence_vector = np.zeros(w2v.vector_size)\n",
    "    word_count = 0\n",
    "    \n",
    "    for word in wordsList:\n",
    "        try:\n",
    "            sentence_vector += w2v[word]\n",
    "            word_count += 1\n",
    "        except:\n",
    "            continue\n",
    "    if word_count != 0:\n",
    "        sentence_vector /= word_count\n",
    "    \n",
    "    return sentence_vector\n",
    "\n",
    "def df_vectorize(path):\n",
    "    \"\"\"Read csv and add vector column.\"\"\"\n",
    "    \n",
    "    semiLearnDf = pd.read_csv(path, engine='python', sep='\\t', header=None, encoding=\"utf-8\")\n",
    "    \n",
    "    if semiLearnDf.shape[1] == 1:\n",
    "        col = {semiLearnDf.columns[0]: \"text\"}\n",
    "    else:\n",
    "        col = {semiLearnDf.columns[0]: \"label\", semiLearnDf.columns[1]: \"text\"}\n",
    "        \n",
    "    semiLearnDf = semiLearnDf.rename(columns=col)\n",
    "    semiLearnDf['vector'] = semiLearnDf.text.str.split().apply(sentence_vectorize)\n",
    "    return semiLearnDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def cv_score(clf, X, y, metric, cv=5):\n",
    "    scores = cross_val_score(clf, X, y, cv=cv, scoring=metric, n_jobs=1)\n",
    "    print(\"%s: %0.2f (+/- %0.2f)\" % (metric, scores.mean(), scores.std() * 2))\n",
    "    return scores.mean()\n",
    "\n",
    "def train_model(clf, X, y):\n",
    "    score = cv_score(clf, X, y, metric='f1_macro')\n",
    "    clf.fit(X, y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Исключение доменов, которые уже есть в обучении\n",
    "def remove_marked(df, textsTrained):\n",
    "    return df.loc[~df['text'].isin(textsTrained)].reset_index(drop=True)\n",
    "\n",
    "# Вероятности для записей датафрейма\n",
    "def get_proba(df, clf, textsTrained):\n",
    "    \n",
    "    # Собираем отдельный df для векторов текстов\n",
    "    VectorDf = pd.DataFrame(df.vector.tolist())\n",
    "    \n",
    "    # Записываем уровень уверенности для каждого домена (с округлением)\n",
    "    predictArray = clf.predict_proba(VectorDf)\n",
    "    labels = [np.argmax(x) for x in predictArray]\n",
    "    \n",
    "    # Фиксируем самый вероятный класс\n",
    "    df['label'] = [clf.classes_[x] for x in labels]\n",
    "    df['proba'] = [max(x) for x in predictArray]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Отбираем записи по порогу t\n",
    "def get_confidence(semiLearnDf, df_vect, t, delta):\n",
    "    \n",
    "    df_vect_confidence = df_vect[df_vect['proba'] >= t].reset_index(drop=True).drop(columns='proba')\n",
    "    \n",
    "    # Перекидываем колонку label на первое место для будущего сложения\n",
    "    cols = df_vect_confidence.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df_vect_confidence = df_vect_confidence[cols]\n",
    "    \n",
    "    length = len(df_vect_confidence)\n",
    "    \n",
    "    # Записываем, в чем уверены\n",
    "    if length > delta:    \n",
    "        semiLearnDf = semiLearnDf.append(df_vect_confidence, ignore_index=True)\n",
    "    \n",
    "    return semiLearnDf, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def second_step(word):\n",
    "#     semiLearnFilepath = folder + word + '.csv'\n",
    "#     semiLearnDf = df_vectorize(semiLearnFilepath)\n",
    "#     textFilepath = 'Data\\\\Non-Annotated\\\\' + word + '.csv'\n",
    "#     textDfUnmarked = df_vectorize(textFilepath)\n",
    "    \n",
    "#     semiLearnDf = semi_learn_w2v(semiLearnDf, textDfUnmarked)\n",
    "#     return semiLearnDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_learn_w2v(semiLearnDf, textDfUnmarked):\n",
    "\n",
    "    while True:\n",
    "        # Обучаем классификатор на размеченном тексте\n",
    "        trainVectorDf = pd.DataFrame(semiLearnDf.vector.tolist())\n",
    "        model = RandomForestClassifier(criterion='entropy', n_estimators=300, \n",
    "                                     max_depth=5, min_samples_leaf=5, min_samples_split=3)\n",
    "\n",
    "        clf = train_model(model, trainVectorDf, semiLearnDf.label)\n",
    "\n",
    "        # Пересобираем набор размеченных записей\n",
    "        textsTrained = semiLearnDf.text.tolist()\n",
    "\n",
    "        # Находим вероятности для неразмеченных записей\n",
    "        df_vect = get_proba(textDfUnmarked, clf, textsTrained)\n",
    "\n",
    "        # Записываем уверенные записи и удаляем их из неразмеченных\n",
    "        semiLearnDf, delta = get_confidence(semiLearnDf, df_vect, 0.90, 500)\n",
    "        \n",
    "        print('Delta =', delta)\n",
    "        \n",
    "        if delta <= 500:\n",
    "            break\n",
    "        else:\n",
    "            textDfUnmarked = remove_marked(textDfUnmarked, textsTrained)\n",
    "            \n",
    "    return semiLearnDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_word(word, semiLearnDf):\n",
    "    filepath = 'Data\\\\Expanded w2v\\\\' + '\\\\Original\\\\' + word + '.csv'\n",
    "    resultDf = semiLearnDf.drop(columns='vector')\n",
    "    resultDf.to_csv(filepath, sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'urllib' has no attribute 'urlretrieve'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c0a0fae541cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://panchenko.me/data/dsl-backup/w2v-ru/all.norm-sz100-w10-cb0-it1-min100.w2v\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"all.norm-sz100-w10-cb0-it1-min100.w2v\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'urllib' has no attribute 'urlretrieve'"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "\n",
    "filename = \"http://panchenko.me/data/dsl-backup/w2v-ru/all.norm-sz100-w10-cb0-it1-min100.w2v\"\n",
    "urllib.urlretrieve (filename, \"all.norm-sz100-w10-cb0-it1-min100.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_fpath = \"all.norm-sz100-w10-cb0-it1-min100.w2v\"\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(w2v_fpath, binary=True, unicode_errors='ignore')\n",
    "w2v.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "проспект\n",
      "f1_macro: 0.27 (+/- 0.10)\n",
      "Delta = 480\n",
      "----------------------------------------------------\n",
      "пытка\n",
      "f1_macro: 0.50 (+/- 0.00)\n",
      "Delta = 1331\n",
      "f1_macro: 0.50 (+/- 0.00)\n",
      "Delta = 1411\n",
      "f1_macro: 0.50 (+/- 0.00)\n",
      "Delta = 98\n",
      "----------------------------------------------------\n",
      "рысь\n",
      "f1_macro: 0.77 (+/- 0.14)\n",
      "Delta = 721\n",
      "f1_macro: 0.76 (+/- 0.14)\n",
      "Delta = 814\n",
      "f1_macro: 0.76 (+/- 0.11)\n",
      "Delta = 155\n",
      "----------------------------------------------------\n",
      "поток\n",
      "f1_macro: 0.61 (+/- 0.10)\n",
      "Delta = 715\n",
      "f1_macro: 0.60 (+/- 0.09)\n",
      "Delta = 812\n",
      "f1_macro: 0.61 (+/- 0.08)\n",
      "Delta = 132\n",
      "----------------------------------------------------\n",
      "проказа\n",
      "f1_macro: 0.52 (+/- 0.11)\n",
      "Delta = 0\n",
      "----------------------------------------------------\n",
      "проспект\n",
      "f1_macro: 0.24 (+/- 0.00)\n",
      "Delta = 458\n",
      "----------------------------------------------------\n",
      "пытка\n",
      "f1_macro: 0.50 (+/- 0.00)\n",
      "Delta = 1329\n",
      "f1_macro: 0.50 (+/- 0.00)\n",
      "Delta = 1414\n",
      "f1_macro: 0.50 (+/- 0.00)\n",
      "Delta = 99\n",
      "----------------------------------------------------\n",
      "рысь\n",
      "f1_macro: 0.76 (+/- 0.13)\n",
      "Delta = 729\n",
      "f1_macro: 0.77 (+/- 0.13)\n",
      "Delta = 810\n",
      "f1_macro: 0.76 (+/- 0.14)\n",
      "Delta = 139\n",
      "----------------------------------------------------\n",
      "среда\n",
      "f1_macro: 0.56 (+/- 0.11)\n",
      "Delta = 2\n",
      "----------------------------------------------------\n",
      "хвост\n",
      "f1_macro: 0.21 (+/- 0.05)\n",
      "Delta = 540\n",
      "f1_macro: 0.22 (+/- 0.05)\n",
      "Delta = 744\n",
      "f1_macro: 0.22 (+/- 0.05)\n",
      "Delta = 354\n",
      "----------------------------------------------------\n",
      "штамп\n",
      "f1_macro: 0.24 (+/- 0.00)\n",
      "Delta = 239\n",
      "----------------------------------------------------\n",
      "Wall time: 8min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for word in wordList:\n",
    "    print(word)\n",
    "    semiLearnDf = second_step(word)\n",
    "    write_word(word, semiLearnDf)\n",
    "    print('----------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
